{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "from scipy.linalg import orth\n",
    "from numpy.linalg import eig\n",
    "from numpy.linalg import svd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AdaM: Adaptive Moments Optimizer\n",
    "## Params\n",
    "### alpha0: base learning rate\n",
    "### grad: current gradient\n",
    "### adam_values: dictionary containing moment estimates\n",
    "\n",
    "def get_AdaM_update(alpha_0, grad, adam_values, b1=.95, b2=.999, e=1e-8):\n",
    "    adam_values['t'] += 1\n",
    "\n",
    "    # update mean                                                                                                                                                                                                     \n",
    "    adam_values['mean'] = b1 * adam_values['mean'] + (1-b1) * grad\n",
    "    m_hat = adam_values['mean'] / (1-b1**adam_values['t'])\n",
    "\n",
    "    # update variance                                                                                                                                                                                                 \n",
    "    adam_values['var'] = b2 * adam_values['var'] + (1-b2) * grad**2\n",
    "    v_hat = adam_values['var'] / (1-b2**adam_values['t'])\n",
    "\n",
    "    return alpha_0 * m_hat/(np.sqrt(v_hat) + e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simulate some data to work with...   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEPCAYAAABV6CMBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noolDKaVUlWjiUEopVSX/D3hlMY6TjfLSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10feebed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulate N observations from a cubic function\n",
    "N = 250\n",
    "x = np.array([np.random.uniform(-4,4) for n in range(N)])[np.newaxis].T\n",
    "y = x**3\n",
    "\n",
    "# True function\n",
    "N_true = 100\n",
    "x_true = np.linspace(start=-6, stop=6, num=N_true)\n",
    "y_true = x_true**3\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', label=\"True Function\")\n",
    "plt.plot(x, y, 'xk', ms=10, linewidth=1, label=\"Observations\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling Neural Networks into Local Linear Regressors\n",
    "#### by [Eric Nalisnick](http://www.ics.uci.edu/~enalisni/) \n",
    "\n",
    "Neural network distillation [(Ba and Caruana, 2013)](https://arxiv.org/abs/1312.6184) [(Hinton et al., 2015)](https://arxiv.org/abs/1503.02531) [(Balan et al., 2015)](http://papers.nips.cc/paper/5965-bayesian-dark-knowledge)---where the probabilities produced by a large 'teacher' model are used as targets for training a smaller 'student' model---is crucial for delopying neural networks to systems for which runtime and memory are limited.  This notebook proposes a new strategy for *probabilistic* distillation, providing a distribution over various simpler models.  Moreover, the proposed method has the benefits that (1) the student model does not need its own optimization loop and (2) as the students are mixtures of linear models, they allow for a high degree of interpretability.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Nonparametric Regression\n",
    "We first review [nonparametric regression](http://www.stat.cmu.edu/~larry/=sml/nonpar.pdf) methods.  Recall kernelized nearest neighbors regression, which for an input $\\mathbf{x}$ has the form: $$ \\mathbb{E}[\\mathbf{y} | \\mathbf{x}] \\ \\ = \\ \\ \\boldsymbol{\\mu}^{Y} \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ \\mathbf{y}_{i}  \\ \\ \\ \\text{where} \\ \\ \\ \\pi_{i}(\\mathbf{x}) = \\frac{k(\\mathbf{x}, \\mathbf{x}_{i}; \\beta)}{\\sum_{j=1}^{N} k(\\mathbf{x}, \\mathbf{x}_{j}; \\beta)}.$$  $\\boldsymbol{\\mu}^{Y}$ is the mean of the label/reponse distribution, $\\pi_{i}(\\mathbf{x})$ is a weighting function comprised of kernels, and $\\beta$ is a scale parameter for the kernels.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel(x1, x2, beta=.1):\n",
    "    #if x1[0] == x2[0]: return 1.\n",
    "    #return 0\n",
    "    return np.exp(-np.sum((x1-x2)**2)/beta)\n",
    "\n",
    "\n",
    "kernel_vals = np.zeros((N_true, N))\n",
    "beta_val = .1\n",
    "\n",
    "for i in range(N_true):\n",
    "    for j in range(N):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[j], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEPCAYAAABV6CMBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCJElkjiEEEJkiSQOIYQQWfL/+QydZwZCllwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11004b110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yHat_kernelNN = np.sum(y.T * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, yHat_kernelNN, 'b--', linewidth=9, label=\"Kernel NP Regression\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Transforming Neural Networks into Local Regressors\n",
    "Next consider the problem of compressing a trained feedforward neural network with ReLU activations.  Define the network as $\\boldsymbol{\\mu}^{Y}_{i} = g^{-1}(f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}))$ where $\\boldsymbol{\\mu}^{Y}_{i}$ is the mean of the output distribution for input $\\mathbf{x}_{i}$, $g(\\cdot)$ is the link function, and $f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})$ is the output of the neural network (before the output activation).\n",
    "\n",
    "#### 0th Order Approximation\n",
    "One, possibly naive, idea is to use the predicted outputs within the nearest neighbors regression framework: $$ g(\\boldsymbol{\\mu}^{Y, 0}) \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ g(\\hat{\\mathbf{y}}_{i}) \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})$$ where $\\hat{\\mathbf{y}}_{i} = g^{-1}(f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}))$.  Notice that we have changed the formulation slightly, having the weighted sum being taken over the sufficient statistic (pre-link).  This approach doesn't make much sense, however, since it would be much easier to use the observed labels than training another model and using the predicted ones---although, the model may introduce some regularity / smoothing.\n",
    "\n",
    "#### 1st Order Approximation\n",
    "Yet, we can improve the approximation by incorportaing more information about the function the network learned.  For a test point $\\mathbf{x}$, we can make its corresponding prediction by examining a first-order Taylor expansion around training point $\\mathbf{x}_{i}$: $ f(\\mathbf{x}; \\boldsymbol{\\theta}) \\approx f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}) + (\\mathbf{x} - \\mathbf{x}_{i}) \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}}.$  The resulting local regression model is then:  $$ g(\\boldsymbol{\\mu}^{Y, 1}) \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ [f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}) + (\\mathbf{x} - \\mathbf{x}_{i}) \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}}].$$  However, as pointed out by [Wang et al. (ICML 2016)](http://proceedings.mlr.press/v48/wanga16.pdf),  feedforward ReLU networks have a special property: $f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}) = \\mathbf{x}_{i} \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}}$, and thus we can simplify to: $$ g(\\boldsymbol{\\mu}^{Y, 1}) \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ [f(\\mathbf{x}_{i}; \\boldsymbol{\\theta}) + \\mathbf{x} \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}} - \\mathbf{x}_{i} \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}}] \\ \\ = \\ \\ \\sum_{i=1}^{N} \\pi_{i}(\\mathbf{x}) \\ \\mathbf{x} \\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}} .$$  As we can precompute these first derivatives, storing them as vectors $\\frac{\\partial f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}_{i}} = \\mathbf{u}_{i}$, we have reduced the ReLU network to locally linear functions.  This result isn't too surprising, as it is known that [ReLU networks are piecewise linear](https://arxiv.org/abs/1611.01491). \n",
    "\n",
    "#### 2nd Order Approximation\n",
    "One may then ask, can we further increase the accuracy of the Taylor approximation.  The answer is 'no,' as $\\frac{\\partial^{2} f(\\mathbf{x}_{i}; \\boldsymbol{\\theta})}{\\partial \\mathbf{x}^{2}_{i}} = 0$.  Thus, the first-order approximation is *locally* exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Experiment: Regression Simulation\n",
    "\n",
    "Let's now demonstrate how to find the mixture of experts representation a network trained on one-dimensional data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next let's define the model: a one-hidden-layer neural network with 50 units..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regression NN with one hidden layer of 50 units\n",
    "def logModel(W, b, x, y):\n",
    "    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n",
    "    y_hat = np.dot(h, W[:, 50:].T) + b[:, 50:]\n",
    "    return np.sum( .5 * -(y - y_hat)**2 )\n",
    "\n",
    "def fprop(W, b, x):\n",
    "    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n",
    "    return np.dot(h, W[:, 50:].T) + b[:, 50:]\n",
    "\n",
    "\n",
    "### GET DERIVATIVES ###\n",
    "# d log p(X | \\theta) / d \\theta\n",
    "dLogModel_dW = grad(logModel)\n",
    "dLogModel_db = grad(lambda b, W, x, y: logModel(W, b, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's train the NN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### INIT PARAMS \n",
    "theta = {'W': .001 * np.random.normal(size=(1,100)), 'b': np.zeros((1,51))}\n",
    "\n",
    "### ELBO OPTIMIZATION\n",
    "maxEpochs = 1000\n",
    "learning_rate = .01\n",
    "adam_values = {'W':{'mean': 0., 'var': 0., 't': 0}, \n",
    "               'b':{'mean': 0., 'var': 0., 't': 0}}\n",
    "\n",
    "loss = 0.\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    dModel_dW = dLogModel_dW(theta['W'], theta['b'], x, y) \n",
    "    dModel_db = dLogModel_db(theta['b'], theta['W'], x, y)\n",
    "        \n",
    "    theta['W'] += get_AdaM_update(learning_rate, dModel_dW, adam_values['W'])  \n",
    "    theta['b'] += get_AdaM_update(learning_rate, dModel_db, adam_values['b'])\n",
    "    \n",
    "    loss += logModel(theta['W'], theta['b'], x, y)\n",
    "    if (epochIdx+1) % 100 == 0:\n",
    "        print \"%d. Neg. Log Likelihood: %.3f\" %(epochIdx+1, -loss/5.)\n",
    "        loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And visualize the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x, y, 'xk', ms=10, linewidth=1, label=\"Observations\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=5, label=\"Model\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearizing the Network\n",
    "\n",
    "Next let's write a function that extracts $\\tilde{\\mathbf{U}}_{i}$ for each $\\mathbf{x}_{i}$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_linear_transform(x_i, W, b):\n",
    "    #copy\n",
    "    W_tilde = np.array(W)\n",
    "    b_tilde = np.array(b)\n",
    "    \n",
    "    # fprop\n",
    "    h = np.maximum(np.dot(x_i, W[:, :50]) + b[:, :50], 0.)\n",
    "    \n",
    "    # apply mask\n",
    "    for j in range(50):\n",
    "        if h[0, j] == 0: \n",
    "            W_tilde[:, j] = 0.\n",
    "            b_tilde[:, j] = 0.\n",
    "            \n",
    "    # construct matrices\n",
    "    W1_tilde = np.vstack([W_tilde[:, :50], b_tilde[:, :50]]) #add bias\n",
    "    next_bias = np.hstack([1./x_i, np.zeros((1,1))]).T\n",
    "    W1_tilde = np.hstack([W1_tilde, next_bias]) # add bias for next layer\n",
    "    \n",
    "    W2_tilde = np.vstack([W_tilde[:, 50:].T, b_tilde[:, 50:]]) #add bias\n",
    "    \n",
    "    return np.dot(W1_tilde, W2_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate all the $\\tilde{\\mathbf{U}}$ vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.sort(x, axis=0)\n",
    "\n",
    "U_vectors = []\n",
    "for i in range(N):\n",
    "    U_vectors.append( get_linear_transform(x[i,:][np.newaxis], theta['W'], theta['b'])[:,0] )\n",
    "U_vectors = np.array(U_vectors).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a kernel function and calculate it between all pairs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel(x1, x2, beta=.1):\n",
    "    #if x1[0] == x2[0]: return 1.\n",
    "    #return 0\n",
    "    return np.exp(-np.sum((x1-x2)**2)/beta)\n",
    "\n",
    "kernel_vals = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        kernel_vals[i,j] = kernel(x[i], x[j])\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Result\n",
    "\n",
    "Visualize the result on the training data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_w_bias = np.hstack([x, np.ones((N,1))])\n",
    "yHat_MoE = np.sum(np.dot(x_w_bias, U_vectors) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x, fprop(theta['W'], theta['b'], x), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data Result\n",
    "\n",
    "Visualize the result for the true function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel_vals = np.zeros((100, N))\n",
    "beta_val = .1\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(N):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[j], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTrue_w_bias = np.hstack([x_true[np.newaxis].T, np.ones((100,1))])\n",
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, U_vectors) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Pruning Experts \n",
    "Using the above mixture of experts representation in place of the original model requires $\\mathcal{O}(Nd)$ time, as it depends on the number of training points, which is a dependence we'd like to break.  Hence, we would like form a good approximation using only a subset of the experts: $$ f(\\mathbf{x}; \\{ \\tilde{\\mathbf{U}}_{1}, \\tilde{\\mathbf{U}}_{2}, \\ldots, \\tilde{\\mathbf{U}}_{N} \\}) \\  \\approx  \\  \\sum_{k=1}^{K} \\pi_{k}(\\mathbf{x}) \\ \\mathbf{x}  \\tilde{\\mathbf{U}}_{k}$$ for $K<<N$.  How should we choose the subset?  Below we attempt several different strategies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.  Random Sampling\n",
    "We start with a naive solution: random sampling.  We sample $K$ pairs $\\{\\mathbf{x}_{k}, \\tilde{\\mathbf{U}}_{k} \\}$ without replacement from the set of $N$.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "k_idxs = np.random.choice(N, K)\n",
    "\n",
    "kernel_vals = np.zeros((100, K))\n",
    "beta_val = 1.\n",
    "\n",
    "for i in range(100):\n",
    "    for j, k in enumerate(k_idxs):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[k], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_subset = U_vectors[:, k_idxs]\n",
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, U_subset) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.  Regular Sampling\n",
    "We continue with another naive solution: sampling regular interval.  We sample $K$ pairs $\\{\\mathbf{x}_{k}, \\tilde{\\mathbf{U}}_{k} \\}$ by choosing every $N/k$th ordered point.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "k_idxs = [0] + [k * N/(K-1) for k in range(1, K-1)] + [N-1]\n",
    "print k_idxs\n",
    "\n",
    "kernel_vals = np.zeros((100, K))\n",
    "\n",
    "for i in range(100):\n",
    "    for j, k in enumerate(k_idxs):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[k], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_subset = U_vectors[:, k_idxs]\n",
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, U_subset) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.   Douglas-Peucker Algorithm\n",
    "For polylines, there is a recursive simplification algorithm known as the [Douglas-Peucker Algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm).  It has an expected runtime of $\\mathcal{O}(N \\log N)$.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def timenow():\n",
    "    return int(time.time() * 1000)\n",
    "\n",
    "def sqr(x):\n",
    "    return x*x\n",
    "\n",
    "def distSquared(p1, p2):\n",
    "    return sqr(p1[0] - p2[0]) + sqr(p1[1] - p2[1])\n",
    "\n",
    "class Line(object):\n",
    "    def __init__(self, p1, p2):\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.lengthSquared = distSquared(self.p1, self.p2)\n",
    "\n",
    "    def getRatio(self, point):\n",
    "        segmentLength = self.lengthSquared\n",
    "        if segmentLength == 0:\n",
    "            return distSquared(point, p1);\n",
    "        return ((point[0] - self.p1[0]) * (self.p2[0] - self.p1[0]) + \\\n",
    "        (point[1] - self.p1[1]) * (self.p2[1] - self.p1[1])) / segmentLength\n",
    "\n",
    "    def distanceToSquared(self, point):\n",
    "        t = self.getRatio(point)\n",
    "\n",
    "        if t < 0:\n",
    "            return distSquared(point, self.p1)\n",
    "        if t > 1:\n",
    "            return distSquared(point, self.p2)\n",
    "\n",
    "        return distSquared(point, [\n",
    "            self.p1[0] + t * (self.p2[0] - self.p1[0]),\n",
    "            self.p1[1] + t * (self.p2[1] - self.p1[1])\n",
    "        ])\n",
    "\n",
    "    def distanceTo(self, point):\n",
    "        return math.sqrt(self.distanceToSquared(point))\n",
    "\n",
    "\n",
    "def simplifyDouglasPeucker(points, pointsToKeep):\n",
    "    weights = []\n",
    "    length = len(points)\n",
    "\n",
    "    def douglasPeucker(start, end):\n",
    "        if (end > start + 1):\n",
    "            line = Line(points[start], points[end])\n",
    "            maxDist = -1\n",
    "            maxDistIndex = 0\n",
    "\n",
    "            for i in range(start + 1, end):\n",
    "                dist = line.distanceToSquared(points[i])\n",
    "                if dist > maxDist:\n",
    "                    maxDist = dist\n",
    "                    maxDistIndex = i\n",
    "\n",
    "            weights.insert(maxDistIndex, maxDist)\n",
    "\n",
    "            douglasPeucker(start, maxDistIndex)\n",
    "            douglasPeucker(maxDistIndex, end)\n",
    "\n",
    "    douglasPeucker(0, length - 1)\n",
    "    weights.insert(0, float(\"inf\"))\n",
    "    weights.append(float(\"inf\"))\n",
    "\n",
    "    weightsDescending = weights\n",
    "    weightsDescending = sorted(weightsDescending, reverse=True)\n",
    "\n",
    "    maxTolerance = weightsDescending[pointsToKeep - 1]\n",
    "    result = [\n",
    "        point for i, point in enumerate(points) if weights[i] >= maxTolerance\n",
    "    ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "point_tups = zip(x[:,0].tolist(), fprop(theta['W'], theta['b'], x)[:,0].tolist())\n",
    "points_kept = simplifyDouglasPeucker(points=point_tups, pointsToKeep=K)\n",
    "k_idxs = [point_tups.index(p) for p in points_kept]\n",
    "\n",
    "kernel_vals = np.zeros((100, K))\n",
    "\n",
    "for i in range(100):\n",
    "    for j, k in enumerate(k_idxs):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[k], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_subset = U_vectors[:, k_idxs]\n",
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, U_subset) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.   Sensitivity-Based Pruning\n",
    "Recall that $\\tilde{\\mathbf{U}}_{i} = \\frac{\\partial g(\\boldsymbol{\\mu}^{Y}_{i})}{\\partial \\mathbf{x}_{i}}$, and therefore  $|| \\tilde{\\mathbf{U}}_{i}|| = || \\frac{\\partial g(\\boldsymbol{\\mu}^{Y}_{i})}{\\partial \\mathbf{x}_{i}} ||$.  In other words, the linear transformations with the largest norms are the ones most sensitive to the input $\\mathbf{x}$, changing their output quickly with small changes to the input.  It is reasonable to think that these transformations have the least chance of generalizing and therefore should be the ones that are pruned.  In classification settings, we may prune for each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 8\n",
    "k_idxs = ((np.sum(U_vectors**2, axis=0))).argsort()[N/2-K/2:-(N/2-K/2)]\n",
    "print k_idxs\n",
    "\n",
    "kernel_vals = np.zeros((100, K))\n",
    "\n",
    "for i in range(100):\n",
    "    for j, k in enumerate(k_idxs):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], x[k], beta_val)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_subset = U_vectors[:, k_idxs]\n",
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, U_subset) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.   Low-Rank Representation\n",
    "Instead of pruning $\\tilde{\\mathbf{U}}_{i}$'s, we can simply try to find a low-rank representation that takes all transformations into account: $\\tilde{\\mathbf{U}}_{d \\times N} = \\mathbf{V}_{d \\times K} \\mathbf{Q}_{K \\times N}$.  The $\\mathbf{V}$ matrix can then be used to make predictions.  If $\\tilde{\\mathbf{U}}_{i}$ is a matrix, then we must perform tensor decomposition.  In order to formulate the kernel weighting function, we'll need to change the basis of the original dataset via $\\mathbf{Q}\\mathbf{X} = \\mathbf{Z}_{K \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K=2\n",
    "V,S,Q = svd(U_vectors, full_matrices=False)\n",
    "S = np.diag(S)\n",
    "Z_train = np.dot(Q, x)  # K X d\n",
    "\n",
    "kernel_vals = np.zeros((100, K))\n",
    "k_idxs = [0, 1]\n",
    "for i in range(100):\n",
    "    for j, k in enumerate(k_idxs):\n",
    "        kernel_vals[i,j] = kernel(x_true[i], Z_train[k], 1.)\n",
    "        \n",
    "kernel_vals = kernel_vals / (kernel_vals.sum(axis=1)[np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yHat_MoE = np.sum(np.dot(xTrue_w_bias, V) * kernel_vals, axis=1)\n",
    "\n",
    "plt.plot(x_true, y_true, 'k-', linewidth=5, label=\"True Function\")\n",
    "plt.plot(x_true, fprop(theta['W'], theta['b'], x_true[np.newaxis].T), 'r-', linewidth=7, label=\"Model\")\n",
    "plt.plot(x_true, yHat_MoE, 'b--', linewidth=7, label=\"MoE Approx\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Experiment: MNIST\n",
    "We now test the above pruning strategy on a more realistic example, MNIST.  Let's load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from random import shuffle\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# reduce dataset and normalize to [0,1]\n",
    "random_idxs = range(mnist.data.shape[0])\n",
    "shuffle(random_idxs)\n",
    "train_x = mnist.data[random_idxs[:10000],:] / 255.\n",
    "test_x = mnist.data[random_idxs[10000:15000],:] / 255.\n",
    "train_y = np.zeros((10000, 10))\n",
    "for i, y in enumerate(mnist.target[random_idxs[:10000]]):\n",
    "    train_y[i, int(y)] = 1.\n",
    "test_y = mnist.target[random_idxs[10000:15000]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, input_d = train_x.shape\n",
    "N_test = test_x.shape[0]\n",
    "hidden_d = 250\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "def softmax(z):\n",
    "    y = np.exp(z)\n",
    "    return y / y.sum()\n",
    "\n",
    "# Classification NN with one hidden layer\n",
    "def fprop(W, b, x):\n",
    "    h = np.maximum(np.dot(x, W[:, :input_d * hidden_d].reshape(input_d, hidden_d)) + b[:, :hidden_d], 0.)\n",
    "    return np.dot(h, W[:, input_d * hidden_d:].reshape(hidden_d, n_classes)) + b[:, hidden_d:]\n",
    "\n",
    "def logModel(W, b, x, y):\n",
    "    y_hat = softmax(fprop(W, b, x))\n",
    "    return np.mean( y * np.log(y_hat) )\n",
    "\n",
    "def accuracy(y, y_hat):    \n",
    "    correct = 0.\n",
    "    for y, yh in zip(y, y_hat): \n",
    "        if y == yh: correct += 1\n",
    "    return correct / len(y_hat)\n",
    "\n",
    "\n",
    "### GET DERIVATIVES ###\n",
    "# d log p(X | \\theta) / d \\theta\n",
    "dLogModel_dW = grad(logModel)\n",
    "dLogModel_db = grad(lambda b, W, x, y: logModel(W, b, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Neg. Log Likelihood: 1.008\n",
      "6. Neg. Log Likelihood: 0.972\n",
      "9. Neg. Log Likelihood: 0.960\n",
      "12. Neg. Log Likelihood: 0.953\n",
      "15. Neg. Log Likelihood: 0.948\n",
      "18. Neg. Log Likelihood: 0.944\n",
      "21. Neg. Log Likelihood: 0.941\n",
      "24. Neg. Log Likelihood: 0.938\n",
      "27. Neg. Log Likelihood: 0.936\n",
      "30. Neg. Log Likelihood: 0.935\n",
      "33. Neg. Log Likelihood: 0.933\n",
      "36. Neg. Log Likelihood: 0.932\n",
      "39. Neg. Log Likelihood: 0.931\n",
      "42. Neg. Log Likelihood: 0.930\n",
      "45. Neg. Log Likelihood: 0.929\n",
      "48. Neg. Log Likelihood: 0.928\n"
     ]
    }
   ],
   "source": [
    "### INIT PARAMS \n",
    "theta = {'W': .001 * np.random.normal(size=(1, input_d*hidden_d + hidden_d*n_classes)), 'b': np.zeros((1, hidden_d + n_classes))}\n",
    "\n",
    "### ELBO OPTIMIZATION\n",
    "maxEpochs = 50\n",
    "batchSize = 100\n",
    "learning_rate = .0003\n",
    "adam_values = {'W':{'mean': 0., 'var': 0., 't': 0}, \n",
    "               'b':{'mean': 0., 'var': 0., 't': 0}}\n",
    "\n",
    "loss = 0.\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    for batchIdx in range(10000/batchSize):\n",
    "    \n",
    "        dModel_dW = dLogModel_dW(theta['W'], theta['b'], train_x[batchIdx*batchSize:(batchIdx+1)*batchSize, :], train_y[batchIdx*batchSize:(batchIdx+1)*batchSize, :]) \n",
    "        dModel_db = dLogModel_db(theta['b'], theta['W'], train_x[batchIdx*batchSize:(batchIdx+1)*batchSize, :], train_y[batchIdx*batchSize:(batchIdx+1)*batchSize, :])\n",
    "        \n",
    "        theta['W'] += get_AdaM_update(learning_rate, dModel_dW, adam_values['W'])  \n",
    "        theta['b'] += get_AdaM_update(learning_rate, dModel_db, adam_values['b'])\n",
    "    \n",
    "    loss += logModel(theta['W'], theta['b'], train_x, train_y)\n",
    "    if (epochIdx+1) % 3 == 0:\n",
    "        print \"%d. Neg. Log Likelihood: %.3f\" %(epochIdx+1, -loss/3.)\n",
    "        loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9949\n",
      "Test accuracy: 0.9538\n"
     ]
    }
   ],
   "source": [
    "og_net_train_acc = accuracy(np.argmax(train_y, axis=1).tolist(), np.argmax(softmax(fprop(theta['W'], theta['b'], train_x)), axis=1).tolist())\n",
    "og_net_test_acc = accuracy(test_y, np.argmax(softmax(fprop(theta['W'], theta['b'], test_x)), axis=1).tolist())\n",
    "\n",
    "print \"Train accuracy: %.4f\" %(og_net_train_acc)\n",
    "print \"Test accuracy: %.4f\" %(og_net_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_linear_transform(x_i, W, b):\n",
    "    #copy\n",
    "    W1_tilde = np.array(W[:, :input_d * hidden_d].reshape(input_d, hidden_d))\n",
    "    W2 = np.array(W[:, input_d * hidden_d:].reshape(hidden_d, n_classes))\n",
    "    b_tilde = np.array(b)\n",
    "    \n",
    "    # fprop\n",
    "    h = np.maximum(np.dot(x_i, W1_tilde) + b_tilde[:, :hidden_d], 0.)\n",
    "    \n",
    "    # apply mask\n",
    "    for j in range(hidden_d):\n",
    "        if h[0, j] == 0: \n",
    "            W1_tilde[:, j] = np.zeros((input_d))\n",
    "            b_tilde[:, j] = 0.\n",
    "            \n",
    "    # construct matrices\n",
    "    W1_tilde = np.vstack([W1_tilde, b_tilde[:, :hidden_d]]) #add bias\n",
    "    next_bias = np.hstack([np.ones((1, input_d))/x_i.sum(), np.zeros((1,1))]).T \n",
    "    W1_tilde = np.hstack([W1_tilde, next_bias]) # add bias for next layer\n",
    "    \n",
    "    W2_tilde = np.vstack([W2, b_tilde[:, hidden_d:]]) #add bias\n",
    "    \n",
    "    return np.dot(W1_tilde, W2_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U_vectors = []\n",
    "for i in range(N):\n",
    "    U_vectors.append( get_linear_transform(train_x[i,:][np.newaxis], theta['W'], theta['b']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_w_bias = np.hstack([train_x, np.ones((N,1))])\n",
    "test_x_w_bias = np.hstack([test_x, np.ones((N_test,1))])\n",
    "Ks = [1, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "all_train = []\n",
    "all_test = []\n",
    "all_train_knn = []\n",
    "all_test_knn = []\n",
    "all_train_fo = []\n",
    "all_test_fo = []\n",
    "\n",
    "for K in Ks:\n",
    "    k_idxs = []\n",
    "    train_labels = np.argmax(train_y, axis=1).tolist()\n",
    "    counter = np.ones((n_classes,)) * K\n",
    "    for k, val in enumerate(train_labels):\n",
    "        if counter[val] > 0: \n",
    "            k_idxs.append(k)\n",
    "            counter[val] -= 1\n",
    "        if counter.sum() == 0: break\n",
    "\n",
    "    beta_val = 5.\n",
    "    train_kernel_vals = np.zeros((N, K*n_classes))\n",
    "    test_kernel_vals = np.zeros((N_test, K*n_classes))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j,k in enumerate(k_idxs):\n",
    "            train_kernel_vals[i,j] = kernel(train_x[i], train_x[k], beta_val)\n",
    "                \n",
    "    for i in range(N_test):\n",
    "        for j,k in enumerate(k_idxs):\n",
    "            test_kernel_vals[i,j] = kernel(test_x[i], train_x[k], beta_val)\n",
    "        \n",
    "    train_kernel_vals = train_kernel_vals / (train_kernel_vals.sum(axis=1)[np.newaxis].T)\n",
    "    test_kernel_vals = test_kernel_vals / (test_kernel_vals.sum(axis=1)[np.newaxis].T)\n",
    "    \n",
    "    U_subset = [U_vectors[k] for k in k_idxs]\n",
    "\n",
    "    temp1 = 0.\n",
    "    temp2 = 0.\n",
    "    temp3 = 0.\n",
    "    temp4 = 0.\n",
    "    temp5 = 0.\n",
    "    for k, U in enumerate(U_subset):\n",
    "        temp1 += np.dot(train_x_w_bias, U) * train_kernel_vals[:,k][np.newaxis].T\n",
    "        temp2 += np.dot(test_x_w_bias, U) * test_kernel_vals[:,k][np.newaxis].T\n",
    "        \n",
    "    temp3 = np.dot(train_y[k_idxs, :].T, train_kernel_vals.T).T\n",
    "    temp4 = np.dot(train_y[k_idxs, :].T, test_kernel_vals.T).T\n",
    "    temp5 = softmax(np.dot(fprop(theta['W'], theta['b'], train_x[k_idxs,:]).T, train_kernel_vals.T).T)\n",
    "    temp6 = softmax(np.dot(fprop(theta['W'], theta['b'], train_x[k_idxs,:]).T, test_kernel_vals.T).T)\n",
    "\n",
    "    train_yHat_MoE = softmax(temp1)\n",
    "    test_yHat_MoE = softmax(temp2)\n",
    "\n",
    "    moe_train_acc = accuracy(np.argmax(train_y, axis=1).tolist(), np.argmax(train_yHat_MoE, axis=1).tolist())\n",
    "    moe_test_acc = accuracy(test_y, np.argmax(test_yHat_MoE, axis=1).tolist())\n",
    "    knn_train_acc = accuracy(np.argmax(train_y, axis=1).tolist(), np.argmax(temp3, axis=1).tolist())\n",
    "    knn_test_acc = accuracy(test_y, np.argmax(temp4, axis=1).tolist())\n",
    "    fo_train_acc = accuracy(np.argmax(train_y, axis=1).tolist(), np.argmax(temp5, axis=1).tolist())\n",
    "    fo_test_acc = accuracy(test_y, np.argmax(temp6, axis=1).tolist())\n",
    "\n",
    "    print \"MoE: Train accuracy for K=%d: %.4f\" %(K, moe_train_acc)\n",
    "    print \"MoE: Test accuracy for K=%d: %.4f\" %(K, moe_test_acc)\n",
    "    print \"KNN: Train accuracy for K=%d: %.4f\" %(K, knn_train_acc)\n",
    "    print \"KNN: Test accuracy for K=%d: %.4f\" %(K, knn_test_acc)\n",
    "    print \"FO: Train accuracy for K=%d: %.4f\" %(K, fo_train_acc)\n",
    "    print \"FO: Test accuracy for K=%d: %.4f\" %(K, fo_test_acc)\n",
    "    print\n",
    "    \n",
    "    all_train.append(moe_train_acc)\n",
    "    all_test.append(moe_test_acc)\n",
    "    all_train_knn.append(knn_train_acc)\n",
    "    all_test_knn.append(knn_test_acc)\n",
    "    all_train_fo.append(fo_train_acc)\n",
    "    all_test_fo.append(fo_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### LINEAR MODEL BASELINE ###\n",
    "# Classification NN with one hidden layer\n",
    "def fprop_linear(W, b, x):\n",
    "    return softmax(np.dot(x, W.reshape(input_d, n_classes)) + b)\n",
    "\n",
    "def logModel_linear(W, b, x, y):\n",
    "    y_hat = fprop_linear(W, b, x)\n",
    "    return np.mean( y * np.log(y_hat) )\n",
    "\n",
    "\n",
    "### GET DERIVATIVES ###\n",
    "# d log p(X | \\theta) / d \\theta\n",
    "dLogModel_linear_dW = grad(logModel_linear)\n",
    "dLogModel_linear_db = grad(lambda b, W, x, y: logModel_linear(W, b, x, y))\n",
    "\n",
    "### INIT PARAMS \n",
    "theta = {'W': .001 * np.random.normal(size=(1, input_d*n_classes)), 'b': np.zeros((1, n_classes))}\n",
    "\n",
    "### ELBO OPTIMIZATION\n",
    "maxEpochs = 100\n",
    "batchSize = 100\n",
    "learning_rate = .0003\n",
    "adam_values = {'W':{'mean': 0., 'var': 0., 't': 0}, \n",
    "               'b':{'mean': 0., 'var': 0., 't': 0}}\n",
    "\n",
    "loss = 0.\n",
    "for epochIdx in range(maxEpochs):\n",
    "    \n",
    "    for batchIdx in range(10000/batchSize):\n",
    "    \n",
    "        dModel_dW = dLogModel_linear_dW(theta['W'], theta['b'], train_x[batchIdx*batchSize:(batchIdx+1)*batchSize, :], train_y[batchIdx*batchSize:(batchIdx+1)*batchSize, :]) \n",
    "        dModel_db = dLogModel_linear_db(theta['b'], theta['W'], train_x[batchIdx*batchSize:(batchIdx+1)*batchSize, :], train_y[batchIdx*batchSize:(batchIdx+1)*batchSize, :])\n",
    "        \n",
    "        theta['W'] += get_AdaM_update(learning_rate, dModel_dW, adam_values['W'])  \n",
    "        theta['b'] += get_AdaM_update(learning_rate, dModel_db, adam_values['b'])\n",
    "    \n",
    "    loss += logModel_linear(theta['W'], theta['b'], train_x, train_y)\n",
    "    if (epochIdx+1) % 3 == 0:\n",
    "        print \"%d. Neg. Log Likelihood: %.3f\" %(epochIdx+1, -loss/3.)\n",
    "        loss = 0.\n",
    "        \n",
    "baseline_train_acc = accuracy(np.argmax(train_y, axis=1).tolist(), np.argmax(fprop_linear(theta['W'], theta['b'], train_x), axis=1).tolist())\n",
    "baseline_test_acc = accuracy(test_y, np.argmax(fprop_linear(theta['W'], theta['b'], test_x), axis=1).tolist())\n",
    "\n",
    "print\n",
    "print \"Train accuracy: %.4f\" %(baseline_train_acc)\n",
    "print \"Test accuracy: %.4f\" %(baseline_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot([k*10 for k in Ks], [og_net_train_acc]*len(Ks), 'b--', linewidth=5, label=\"Original NN, Train\")\n",
    "plt.plot([k*10 for k in Ks], [og_net_test_acc]*len(Ks), 'k--', linewidth=5, label=\"Original NN\")\n",
    "\n",
    "#plt.plot([k*10 for k in Ks], [baseline_train_acc]*len(Ks), 'b:', linewidth=5, label=\"Linear Model, Train\")\n",
    "plt.plot([k*10 for k in Ks], [baseline_test_acc]*len(Ks), 'k:', linewidth=5, label=\"Linear Model\")\n",
    "\n",
    "plt.plot([k*10 for k in Ks], all_test_knn, 'b-', linewidth=5, label=\"Nonparam. Regression\")\n",
    "\n",
    "#plt.plot([k*10 for k in Ks], all_train_knn, 'c-', linewidth=5, label=\"Local Regressors, Order 0, Train\")\n",
    "plt.plot([k*10 for k in Ks], all_test_fo, 'r--', linewidth=5, label=\"Local Regressors, Order 0\")\n",
    "\n",
    "#plt.plot([k*10 for k in Ks], all_train, 'b-', linewidth=5, label=\"Local Regressors, Order 1, Train\")\n",
    "plt.plot([k*10 for k in Ks], all_test, 'r-', linewidth=5, label=\"Local Regressors, Order 1\")\n",
    "\n",
    "plt.xlabel(\"Number of Neighbors / Regressors (K)\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim([.6, 1])\n",
    "\n",
    "plt.title(\"MNIST: Distilling NN to Linear System\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Experiment: Pruning via DPP Sampling\n",
    "Next, let's test how DPP sampling performs in comparison to uniform sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dpp_functions import *\n",
    "\n",
    "train_x_w_bias = np.hstack([train_x, np.ones((N,1))])\n",
    "test_x_w_bias = np.hstack([test_x, np.ones((N_test,1))])\n",
    "Ks = [1, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "y_hats_for_kernel = np.zeros((10, train_x_w_bias.shape[0], 10))\n",
    "counters = np.zeros((10,))\n",
    "y_list = np.argmax(train_y, axis=1).tolist()\n",
    "for idx, U in enumerate(U_vectors):\n",
    "    y_hats_for_kernel[y_list[idx], :, :] += np.dot(train_x_w_bias, U)\n",
    "    counters[y_list[idx]] += 1\n",
    "\n",
    "for idx in range(10):\n",
    "    y_hats_for_kernel[idx, :, :] /= counters[idx]\n",
    "\n",
    "all_train_dpp = []\n",
    "all_test_dpp = []\n",
    "for K in Ks:\n",
    "    \n",
    "    k_idxs = []\n",
    "    for idx in range(n_classes):\n",
    "        k_idxs += sample_from_dpp(y_hats_for_kernel[idx, :, :], K).tolist()\n",
    "\n",
    "    beta_val = 5.\n",
    "    #train_kernel_vals = np.zeros((N, K*n_classes))\n",
    "    test_kernel_vals = np.zeros((N_test, K*n_classes))\n",
    "\n",
    "    #for i in range(N):\n",
    "    #    for j,k in enumerate(k_idxs):\n",
    "    #        train_kernel_vals[i,j] = kernel(train_x[i], train_x[k], beta_val)\n",
    "                \n",
    "    for i in range(N_test):\n",
    "        for j,k in enumerate(k_idxs):\n",
    "            test_kernel_vals[i,j] = kernel(test_x[i], train_x[k], beta_val)\n",
    "            \n",
    "    test_kernel_vals = test_kernel_vals / (test_kernel_vals.sum(axis=1)[np.newaxis].T)\n",
    "    U_subset = [U_vectors[k] for k in k_idxs]\n",
    "\n",
    "    temp1 = 0.\n",
    "    for k, U in enumerate(U_subset):\n",
    "        temp1 += np.dot(test_x_w_bias, U) * test_kernel_vals[:,k][np.newaxis].T\n",
    "\n",
    "    test_yHat_dpp = softmax(temp1)\n",
    "\n",
    "    dpp_test_acc = accuracy(test_y, np.argmax(test_yHat_dpp, axis=1).tolist())\n",
    "    \n",
    "    print \"DPP: Test accuracy for K=%d: %.4f\" %(K, dpp_test_acc)\n",
    "    print\n",
    "    \n",
    "    #all_train.append(moe_train_acc)\n",
    "    all_test_dpp.append(moe_test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
