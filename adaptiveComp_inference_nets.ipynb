{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Networks with Adaptive Computation\n",
    "\n",
    "This notebook outlines an idea for using RNN's [with adaptive computation](https://arxiv.org/abs/1603.08983) to produce [mixture posteriors](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.5846&rep=rep1&type=pdf) for [Density networks](http://www.inference.org.uk/mackay/ch_learning.pdf).\n",
    "\n",
    "### 1.  Variational Inference with Mixture Approximations\n",
    "\n",
    "Recall the method of variational inference.  For an approximation--$q(\\boldsymbol{\\theta};\\boldsymbol{\\phi})$--we fit it using the [*Evidence Lower Bound* (ELBO)](http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf): $$ \\log p(x) = \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}] + \\text{KLD}[q(\\boldsymbol{\\theta}) || p(\\boldsymbol{\\theta} | \\mathbf{X})] \\ge \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}].$$\n",
    "\n",
    "Often the posteriors we encounter in the wild are multi-modal, and thus using uni-modal posterior approximations can drastically misrepresent the posterior's support.  A straight-forward extentension is to use mixture approximations of the form: $$ q(\\boldsymbol{\\theta}) = \\sum_{k} \\pi_{k} q_{k}(\\boldsymbol{\\theta}) $$ where $\\pi_{k}$ is a mixture weight and $q_{k}$ is the kth component density.  Using this approximation, the ELBO can be derived as $$ \\log p(\\mathbf{X}) \\ge \\sum_{k} \\pi_{k} \\mathbb{E}_{q_{k}(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}] $$ where $\\mathbb{H}_{q}[\\boldsymbol{\\theta}] =  -\\sum_{k} \\pi_{k} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) \\log \\sum_{j} \\pi_{j} q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta}$, the entropy of the mixture model.  Unfortunately, this quantity is intractable, but we can use the following lower bound proposed by [Gershman et al. (2012)](https://arxiv.org/pdf/1206.4665.pdf): $$ -\\sum_{k} \\pi_{k} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) \\log \\sum_{j} \\pi_{j} q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta} \\ge -\\sum_{k} \\pi_{k}  \\log \\sum_{j} \\pi_{j} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta}, $$ which is attained via straight forward application of [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).  For most common densities, the convolution above can be solved in closed form.  For Gaussians, we have: $$ \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta} = \\mathbb{E}_{q_{k}}[q_{j}] = \\text{N}(\\boldsymbol{\\mu}_{k}; \\boldsymbol{\\mu}_{j}, (\\boldsymbol{\\sigma}_{k}^{2} + \\boldsymbol{\\sigma}_{j}^{2})\\mathbb{I} ).$$  Intuitively, the convolution term (entropy lower bound) can be seen as encouraging the components to have low probability under one another.  Or in other words, to spread out and diversify their posterior coverage.  Plugging the Gaussian solution back into the ELBO lower bound we have: $$\\log p(\\mathbf{X}) \\ge \\mathcal{L}_{\\text{ELBO}} \\ge   \\sum_{k} \\pi_{k} \\mathbb{E}_{q_{k}(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + -\\sum_{k} \\pi_{k}  \\log \\sum_{j} \\pi_{j} \\text{N}(\\boldsymbol{\\mu}_{k}; \\boldsymbol{\\mu}_{j}, (\\boldsymbol{\\sigma}_{k}^{2} + \\boldsymbol{\\sigma}_{j}^{2})\\mathbb{I} ).$$ \n",
    "\n",
    "\n",
    "### 2.  Adaptive Inference with RNNs\n",
    "\n",
    "When using models with per-data-point latent variables of the form $$ \\mathbf{z}_{i} \\sim p(\\mathbf{z}), \\ \\ \\mathbf{x}_{i} \\sim p(\\mathbf{x}_{i} | \\mathbf{z}_{i}),$$ it may be useful to have per-data-point posteriors $q(\\mathbf{z}_{i} | \\mathbf{x}_{i})$ of varying complexity.  For example, some data points may look like a blend of two different types (for example, a digit that looks like both a one and a seven) and we may wish to place posterior mass on the region of latent space for each digit.  We can define inference networks with this behavior by using an adaptive computation RNN as the inference network.  We propose a network with the following form: $$ \\mathbf{h}_{i, t=k} = f_{\\text{LSTM}}(\\mathbf{h}_{i, t=k-1}, \\mathbf{x}_{i}), \\ \\ \\  \\boldsymbol{\\mu}_{i,k} = f_{\\mu}(\\mathbf{h}_{i,t=k}), \\ \\ \\  \\boldsymbol{\\sigma}_{i,k} = f_{\\sigma}(\\mathbf{h}_{i,t=k}), \\ \\ \\ \\gamma_{i,k} = f_{\\gamma}(\\mathbf{h}_{i,t=k}).$$  The first variable $\\mathbf{h}_{i,t=k}$ is simply the RNN's hidden state at time step $k$, as generated by an LSTM unit taking the previous hidden state $\\mathbf{h}_{i,t=k-1}$ and the data point $\\mathbf{x}_{i}$ as input.  The last three variables are components of the variational mixture approximation.  The first two--$\\boldsymbol{\\mu}_{i,k}$ and $\\boldsymbol{\\sigma}_{i,k}$--are the parameters of the kth Gaussian component and are computed via independent functions of the current hidden state.  The last variable is $\\gamma_{i,k}$, taking on a value $(0,1)$, and thus would have a logistic function output.  We compose the $\\gamma_{i,k}$ variables into the mixture components by using the Dirichlet's [stick breaking construction](https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process): $$ \\pi_{i,1} = \\gamma_{i,1}, \\ \\ \\  \\pi_{i,k} = \\gamma_{i,k} \\prod_{j=1}^{k-1} (1-\\gamma_{i,j}).$$  The RNN will hault computation when the remaining stick is less than some threshold, i.e. $\\prod_{k=1}^{K} (1-\\gamma_{k}) < \\epsilon$.  Following [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), we assume mixtures with few components are preferred, and thus penalize the RNN's computation according to entropy of the mixture weights: $$\\mathbb{H}[\\boldsymbol{\\pi}_{i}] = -\\sum_{k=1}^{K} \\pi_{i,k} \\log \\pi_{i,k} = -\\sum_{k=1}^{K} \\gamma_{i,k} \\prod_{j=1}^{k-1} (1-\\gamma_{i,j}) [ \\log \\gamma_{i,k} + \\sum_{j=1}^{k-1} \\log(1 - \\gamma_{i,j}) ].$$\n",
    "\n",
    "Plugging this term back into the ELBO lower bound in part #1, we have our final objective for the $i$th data point: $$ \\mathcal{L}_{\\text{ADAPT}}(\\mathbf{x}_{i}) = \\sum_{k} \\pi_{i,k} \\mathbb{E}_{q_{k}(\\mathbf{z}_{i} | \\mathbf{x}_{i})}[ \\log p(\\mathbf{x}_{i}, \\mathbf{z}_{i})] - \\sum_{k} \\pi_{i,k}  \\log \\sum_{j} \\pi_{i,j} \\text{N}(\\boldsymbol{\\mu}_{i,k}; \\boldsymbol{\\mu}_{i,j}, (\\boldsymbol{\\sigma}_{i,k}^{2} + \\boldsymbol{\\sigma}_{i,j}^{2})\\mathbb{I} ) - \\mathbb{H}[\\boldsymbol{\\pi}_{i}].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Toy Experiment: Approximating a Mixture\n",
    "\n",
    "We will now implement the above ideas for a toy example in which we approximate a mixture density.  Let's first define a simple mixture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "def log_normal_pdf(x, mu, sigma):\n",
    "    d = mu - x\n",
    "    d2 = tf.mul(-1., tf.mul(d,d))\n",
    "    s2 = tf.mul(2., tf.mul(sigma,sigma))\n",
    "    return tf.reduce_sum(tf.div(d2,s2) - tf.log(tf.mul(sigma, 2.506628)))\n",
    "\n",
    "# Gaussian Mixture\n",
    "def log_gaussMix_pdf(z, pi, mu, sigma, K):\n",
    "    s = tf.mul(pi[0], tf.exp(log_normal_pdf(z, mu[0], sigma[0])))\n",
    "    for k in xrange(K-1):\n",
    "        s += tf.mul(pi[k+1], tf.exp(log_normal_pdf(z, mu[k+1], sigma[k+1])))\n",
    "    return tf.log(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'true' posterior params\n",
    "mu_post = [-4., 3.]\n",
    "sigma_post = [1., 3.]\n",
    "pi_post = [.3, .7]\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=(), name=\"init\")\n",
    "log_prob_z = log_gaussMix_pdf(Z, pi_post, mu_post, sigma_post, 2)\n",
    "\n",
    "# get posterior probabilities\n",
    "z_grid = np.linspace(-10., 10., 1000).astype('float32')\n",
    "with tf.Session() as session:\n",
    "    probs_true = np.exp([session.run(log_prob_z, {Z: z}) for z in z_grid])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(z_grid, probs_true, 'b-', linewidth=5, label=\"True Posterior\")\n",
    "\n",
    "plt.xlabel(\"Z\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our adaptive inference RNN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define dimensionality of model components\n",
    "input_d = 1\n",
    "rnn_hidden_d = 5\n",
    "z_space_d = 1\n",
    "\n",
    "# init data variable\n",
    "X = tf.placeholder(\"float\", [None, input_d])\n",
    "\n",
    "# init RNN params\n",
    "params = {'h':{'W':tf.Variable(tf.random_normal([input_d, rnn_hidden_d], stddev=.001)),\n",
    "               'U':tf.Variable(tf.random_normal([rnn_hidden_d, rnn_hidden_d], stddev=.001)),\n",
    "               'b':tf.Variable(tf.zeros([1, rnn_hidden_d]))\n",
    "              },\n",
    "          'mu':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, z_space_d], stddev=.001)),\n",
    "               'b':tf.Variable(tf.zeros([1, z_space_d]))\n",
    "              },\n",
    "          'sigma':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, z_space_d], stddev=.001)),\n",
    "               'b':tf.Variable(tf.zeros([1, z_space_d]))\n",
    "              },\n",
    "          'pi':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, 1], stddev=.001)),\n",
    "               'b':tf.Variable(tf.zeros([1, 1]))\n",
    "              }\n",
    "        }\n",
    "\n",
    "\n",
    "# define variables for recursion\n",
    "mu = tf.zeros([1, z_space_d])\n",
    "sigma = tf.zeros([1, z_space_d])\n",
    "pi = tf.zeros([1, 1])\n",
    "hidden_state = tf.ones([1, rnn_hidden_d])\n",
    "remaining_stick = tf.ones([1, 1])\n",
    "\n",
    "# make lists to track intermediate values\n",
    "mus = []\n",
    "sigmas = []\n",
    "pis = []\n",
    "\n",
    "\n",
    "def prop_RNN(mu, sigma, pi, hidden_state, remaining_stick):\n",
    "\n",
    "    # Defines an Elman network\n",
    "    # TODO: define LSTM cell\n",
    "    hidden_state = \\\n",
    "    tf.nn.relu(tf.matmul(X, params['h']['W']) + tf.matmul(hidden_state, params['h']['U']) + params['h']['b'])\n",
    "\n",
    "    # Output parameters of approximation\n",
    "    mu = tf.matmul(hidden_state, params['mu']['W']) + params['mu']['b']\n",
    "    sigma = tf.nn.softplus(tf.matmul(hidden_state, params['sigma']['W']) + params['sigma']['b'])\n",
    "    gamma = tf.nn.sigmoid(tf.matmul(hidden_state, params['pi']['W']) + params['pi']['b'])\n",
    "    \n",
    "    # Create pi_k via stick-breaking construction\n",
    "    pi = gamma * remaining_stick\n",
    "    remaining_stick = (1.-gamma) * remaining_stick\n",
    "    \n",
    "    # append to trackers, while_loop outputs only final values\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "    pis.append(pi)\n",
    "    \n",
    "    return mu, sigma, pi, hidden_state, remaining_stick\n",
    "\n",
    "\n",
    "stick_eps = .01\n",
    "def test_stick(mu, sigma, pi, hidden_state, remaining_stick):\n",
    "    return tf.reduce_sum(remaining_stick) > stick_eps\n",
    "\n",
    "\n",
    "mu, sigma, pi, hidden_state, remaining_stick = tf.while_loop(cond=test_stick, \\\n",
    "                                                            body=prop_RNN, \\\n",
    "                                                            loop_vars=[mu, sigma, pi, hidden_state, remaining_stick])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our optimization function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data term: \\sum_k \\pi_k E[log p(x,z)]\n",
    "expected_ll = 0.\n",
    "for k, pi in enumerate(pis):\n",
    "    z_sample = mus[k] + sigmas[k] * tf.random_normal(shape=[1, z_space_d])\n",
    "    expected_ll += pi * log_gaussMix_pdf(z_sample, pi_post, mu_post, sigma_post, 2)\n",
    "    \n",
    "# entropy lower bound term : \\sum_k pi_k log \\sum_j pi_j N(mu_k; mu_j, sigma_k**2 + sigma_j**2)\n",
    "ent_lb_term = 0.\n",
    "for k, pi_k in enumerate(pis):\n",
    "    temp_val = 0.\n",
    "    for j, pi_j in enumerate(pis):\n",
    "        temp_val += pi_j * tf.exp(log_normal_pdf(mus[k], mus[j], tf.sqrt(sigmas[k]**2 + sigmas[j]**2)))\n",
    "    ent_lb_term += -pi_k * tf.log(temp_val)\n",
    "    \n",
    "# entropy of mixture weights\n",
    "ent_mix_weights = 0.\n",
    "for pi in pis:\n",
    "    ent_mix_weights += -pi * tf.log(pi)\n",
    "    \n",
    "objective_fn = tf.reduce_sum(expected_ll) + tf.reduce_sum(ent_lb_term) - tf.reduce_sum(ent_mix_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's train our dynamic RNN approximation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set training params\n",
    "n_epochs = 20\n",
    "learning_rate = .03\n",
    "\n",
    "# get the training operator\n",
    "train_model = tf.train.AdamOptimizer(learning_rate).minimize(-objective_fn, \\\n",
    "            var_list=[params['h']['W'], params['h']['U'], params['h']['b'], \n",
    "                     params['mu']['W'], params['mu']['b'],\n",
    "                      params['sigma']['W'], params['sigma']['b'],\n",
    "                      params['pi']['W'], params['pi']['b']])\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # perform update\n",
    "        _, loss = session.run([train_model, objective_fn], feed_dict={X: np.ones((1,1))})\n",
    "        print \"Epoch %d.  VI Objective: %.3f\" %(epoch_idx, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
