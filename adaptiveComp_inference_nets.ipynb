{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Networks with Adaptive Computation\n",
    "\n",
    "This notebook outlines an idea for using RNN's [with adaptive computation](https://arxiv.org/abs/1603.08983) to produce [mixture posteriors](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.5846&rep=rep1&type=pdf) for [Density networks](http://www.inference.org.uk/mackay/ch_learning.pdf).\n",
    "\n",
    "### 1.  Variational Inference with Mixture Approximations\n",
    "\n",
    "Recall the method of variational inference.  For an approximation--$q(\\boldsymbol{\\theta};\\boldsymbol{\\phi})$--we fit it using the [*Evidence Lower Bound* (ELBO)](http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf): $$ \\log p(x) = \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}] + \\text{KLD}[q(\\boldsymbol{\\theta}) || p(\\boldsymbol{\\theta} | \\mathbf{X})] \\ge \\mathbb{E}_{q(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}].$$\n",
    "\n",
    "Often the posteriors we encounter in the wild are multi-modal, and thus using uni-modal posterior approximations can drastically misrepresent the posterior's support.  A straight-forward extentension is to use mixture approximations of the form: $$ q(\\boldsymbol{\\theta}) = \\sum_{k} \\pi_{k} q_{k}(\\boldsymbol{\\theta}) $$ where $\\pi_{k}$ is a mixture weight and $q_{k}$ is the kth component density.  Using this approximation, the ELBO can be derived as $$ \\log p(\\mathbf{X}) \\ge \\sum_{k} \\pi_{k} \\mathbb{E}_{q_{k}(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + \\mathbb{H}_{q}[\\boldsymbol{\\theta}] $$ where $\\mathbb{H}_{q}[\\boldsymbol{\\theta}] =  -\\sum_{k} \\pi_{k} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) \\log \\sum_{j} \\pi_{j} q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta}$, the entropy of the mixture model.  Unfortunately, this quantity is intractable, but we can use the following lower bound proposed by [Gershman et al. (2012)](https://arxiv.org/pdf/1206.4665.pdf): $$ -\\sum_{k} \\pi_{k} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) \\log \\sum_{j} \\pi_{j} q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta} \\ge -\\sum_{k} \\pi_{k}  \\log \\sum_{j} \\pi_{j} \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta}, $$ which is attained via straight forward application of [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).  For most common densities, the convolution above can be solved in closed form.  For Gaussians, we have: $$ \\int_{\\boldsymbol{\\theta}} q_{k}(\\boldsymbol{\\theta}) q_{j}(\\boldsymbol{\\theta}) \\ d \\boldsymbol{\\theta} = \\mathbb{E}_{q_{k}}[q_{j}] = \\text{N}(\\boldsymbol{\\mu}_{k}; \\boldsymbol{\\mu}_{j}, (\\boldsymbol{\\sigma}_{k}^{2} + \\boldsymbol{\\sigma}_{j}^{2})\\mathbb{I} ).$$  Intuitively, the convolution term (entropy lower bound) can be seen as encouraging the components to have low probability under one another.  Or in other words, to spread out and diversify their posterior coverage.  Plugging the Gaussian solution back into the ELBO lower bound we have: $$\\log p(\\mathbf{X}) \\ge \\mathcal{L}_{\\text{ELBO}} \\ge   \\sum_{k} \\pi_{k} \\mathbb{E}_{q_{k}(\\boldsymbol{\\theta})}[ \\log p(\\mathbf{X}, \\boldsymbol{\\theta})] + -\\sum_{k=1}^{K} \\pi_{k}  \\log \\sum_{j=1} \\pi_{j} \\text{N}(\\boldsymbol{\\mu}_{k}; \\boldsymbol{\\mu}_{j}, (\\boldsymbol{\\sigma}_{k}^{2} + \\boldsymbol{\\sigma}_{j}^{2})\\mathbb{I} ).$$ \n",
    "\n",
    "\n",
    "### 2.  Adaptive Inference with RNNs\n",
    "\n",
    "When using models with per-data-point latent variables of the form $$ \\mathbf{z}_{i} \\sim p(\\mathbf{z}), \\ \\ \\mathbf{x}_{i} \\sim p(\\mathbf{x}_{i} | \\mathbf{z}_{i}),$$ it may be useful to have per-data-point posteriors $q(\\mathbf{z}_{i} | \\mathbf{x}_{i})$ of varying complexity.  For example, some data points may look like a blend of two different types (for example, a digit that looks like both a one and a seven) and we may wish to place posterior mass on the region of latent space for each digit.  We can define inference networks with this behavior by using an adaptive computation RNN as the inference network.  We propose a network with the following form: $$ \\mathbf{h}_{i, t=k} = f_{\\text{LSTM}}(\\mathbf{h}_{i, t=k-1}, \\mathbf{x}_{i}), \\ \\ \\  \\boldsymbol{\\mu}_{i,k} = f_{\\mu}(\\mathbf{h}_{i,t=k}), \\ \\ \\  \\boldsymbol{\\sigma}_{i,k} = f_{\\sigma}(\\mathbf{h}_{i,t=k}), \\ \\ \\ \\gamma_{i,k} = f_{\\gamma}(\\mathbf{h}_{i,t=k}).$$  The first variable $\\mathbf{h}_{i,t=k}$ is simply the RNN's hidden state at time step $k$, as generated by an LSTM unit taking the previous hidden state $\\mathbf{h}_{i,t=k-1}$ and the data point $\\mathbf{x}_{i}$ as input.  The last three variables are components of the variational mixture approximation.  The first two--$\\boldsymbol{\\mu}_{i,k}$ and $\\boldsymbol{\\sigma}_{i,k}$--are the parameters of the kth Gaussian component and are computed via independent functions of the current hidden state.  The last variable is $\\gamma_{i,k}$, taking on a value $(0,1)$, and thus would have a logistic function output.  We compose the $\\gamma_{i,k}$ variables into the mixture components by using the Dirichlet's [stick breaking construction](https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process): $$ \\pi_{i,1} = \\gamma_{i,1}, \\ \\ \\  \\pi_{i,k} = \\gamma_{i,k} \\prod_{j=1}^{k-1} (1-\\gamma_{i,j}).$$  The RNN will hault computation when the remaining stick is less than some threshold, i.e. $\\prod_{k=1}^{K} (1-\\gamma_{k}) < \\epsilon$.  Following [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), we assume mixtures with few components are preferred, and thus penalize the RNN's computation according to entropy of the mixture weights: $$\\mathbb{H}[\\boldsymbol{\\pi}_{i}] = -\\sum_{k=1}^{K} \\pi_{i,k} \\log \\pi_{i,k} = -\\sum_{k=1}^{K} \\gamma_{i,k} \\prod_{j=1}^{k-1} (1-\\gamma_{i,j}) [ \\log \\gamma_{i,k} + \\sum_{j=1}^{k-1} \\log(1 - \\gamma_{i,j}) ].$$\n",
    "\n",
    "Plugging this term back into the ELBO lower bound in part #1, we have our final objective for the $i$th data point: $$ \\mathcal{L}_{\\text{ADAPT}}(\\mathbf{x}_{i}) = \\sum_{k} \\pi_{i,k} \\mathbb{E}_{q_{k}(\\mathbf{z}_{i} | \\mathbf{x}_{i})}[ \\log p(\\mathbf{x}_{i}, \\mathbf{z}_{i})] - \\sum_{k=1}^{K} \\pi_{i,k}  \\log \\sum_{j=1}^{K} \\pi_{i,j} \\text{N}(\\boldsymbol{\\mu}_{i,k}-\\boldsymbol{\\mu}_{i,j}; \\mathbf{0}, (\\boldsymbol{\\sigma}_{i,k}^{2} + \\boldsymbol{\\sigma}_{i,j}^{2})\\mathbb{I} ) - \\mathbb{H}[\\boldsymbol{\\pi}_{i}].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Toy Experiment: Approximating a Mixture\n",
    "\n",
    "We will now implement the above ideas for a toy example in which we approximate a mixture density.  Let's first define a simple mixture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian\n",
    "def log_normal_pdf(x, mu, sigma):\n",
    "    d = mu - x\n",
    "    d2 = tf.mul(-1., tf.mul(d,d))\n",
    "    s2 = tf.mul(2., tf.mul(sigma,sigma))\n",
    "    return tf.reduce_sum(tf.div(d2,s2) - tf.log(tf.mul(sigma, 2.506628)))\n",
    "\n",
    "# Gaussian Mixture\n",
    "def log_gaussMix_pdf(z, pi, mu, sigma, K):\n",
    "    s = tf.mul(pi[0], tf.exp(log_normal_pdf(z, mu[0], sigma[0])))\n",
    "    for k in xrange(K-1):\n",
    "        s += tf.mul(pi[k+1], tf.exp(log_normal_pdf(z, mu[k+1], sigma[k+1])))\n",
    "    return tf.log(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'true' posterior params\n",
    "mu_post = [-4., 3.]\n",
    "sigma_post = [1., 3.]\n",
    "pi_post = [.3, .7]\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=(), name=\"init\")\n",
    "log_prob_z = log_gaussMix_pdf(Z, pi_post, mu_post, sigma_post, 2)\n",
    "\n",
    "# get posterior probabilities\n",
    "z_grid = np.linspace(-10., 10., 1000).astype('float32')\n",
    "with tf.Session() as session:\n",
    "    probs_true = np.exp([session.run(log_prob_z, {Z: z}) for z in z_grid])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(z_grid, probs_true, 'b-', linewidth=5, label=\"True Posterior\")\n",
    "\n",
    "plt.xlabel(\"Z\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our adaptive inference RNN, starting with the recurrent component..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define dimensionality of model components\n",
    "input_d = 1\n",
    "rnn_hidden_d = 5\n",
    "z_space_d = 1\n",
    "\n",
    "max_n_loops = 10\n",
    "\n",
    "# init data variable\n",
    "X = tf.placeholder(\"float\", [None, input_d])\n",
    "\n",
    "# init RNN params\n",
    "params = {'h':{'W':tf.Variable(tf.random_normal([input_d, rnn_hidden_d], stddev=.00001)),\n",
    "               'U':tf.Variable(tf.random_normal([rnn_hidden_d, rnn_hidden_d], stddev=.00001)),\n",
    "               'b':tf.Variable(tf.zeros([rnn_hidden_d,]))\n",
    "              },\n",
    "          'mu':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, z_space_d], stddev=.00001)),\n",
    "               'b':tf.Variable(tf.zeros([z_space_d,]))\n",
    "              },\n",
    "          'sigma':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, z_space_d], stddev=.00001)),\n",
    "               'b':tf.Variable(tf.zeros([z_space_d,]))\n",
    "              },\n",
    "          'pi':{'W':tf.Variable(tf.random_normal([rnn_hidden_d, 1], stddev=.00001)),\n",
    "               'b':tf.Variable(tf.zeros([1,]))\n",
    "              }\n",
    "        }\n",
    "\n",
    "\n",
    "### DEFINE RNN LOOP ###\n",
    "def prop_RNN(hidden_state):\n",
    "\n",
    "    # Defines an Elman network\n",
    "    # TODO: define LSTM cell\n",
    "    hidden_state = \\\n",
    "    tf.nn.relu(tf.matmul(X, params['h']['W']) + tf.matmul(hidden_state, params['h']['U']) + params['h']['b'])\n",
    "    \n",
    "    return hidden_state\n",
    "\n",
    "hidden_states = [tf.zeros([tf.shape(X)[0], rnn_hidden_d])]\n",
    "\n",
    "for loop_idx in range(max_n_loops):\n",
    "    hidden_states.append( prop_RNN(hidden_states[-1]) )\n",
    "    \n",
    "# remove inital state\n",
    "hidden_states = hidden_states[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take the hidden states and determine how many components to compute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pis = []\n",
    "mus = []\n",
    "sigmas = []\n",
    "remaining_stick = 1.\n",
    "\n",
    "stick_eps = .01\n",
    "for idx in range(max_n_loops):\n",
    "    \n",
    "    # compute component params\n",
    "    mus.append(tf.matmul(hidden_states[idx], params['mu']['W']) + params['mu']['b'])\n",
    "    sigmas.append(tf.nn.softplus(tf.matmul(hidden_states[idx], params['sigma']['W']) + params['sigma']['b']))\n",
    "    \n",
    "    # compute component weights\n",
    "    gamma = tf.nn.sigmoid(tf.matmul(hidden_states[idx], params['pi']['W']) + params['pi']['b'])\n",
    "    pis.append(gamma * remaining_stick)\n",
    "    \n",
    "    # update \n",
    "    remaining_stick = (1.-gamma) * remaining_stick\n",
    "    remaining_stick = tf.cond( tf.reduce_max(remaining_stick) < stick_eps, lambda: 0.*remaining_stick, lambda: remaining_stick )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our optimization function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data term: \\sum_k \\pi_k E[log p(x,z)]\n",
    "expected_ll = 0.\n",
    "pi_sum_tracker = 0.\n",
    "for k in range(max_n_loops):\n",
    "    z_sample = mus[k] + sigmas[k] * tf.random_normal(shape=[tf.shape(X)[0], z_space_d])\n",
    "    expected_ll += pis[k] * log_gaussMix_pdf(z_sample, pi_post, mu_post, sigma_post, 2)\n",
    "    \n",
    "# entropy lower bound term : \\sum_k pi_k log \\sum_j pi_j N(mu_k; mu_j, sigma_k**2 + sigma_j**2)\n",
    "ent_lb_term = 0.\n",
    "for k in range(max_n_loops):\n",
    "    temp_val = 0.\n",
    "    for j in range(max_n_loops):\n",
    "        temp_val += pis[j] * tf.exp(log_normal_pdf(mus[k], mus[j], tf.sqrt(sigmas[k]**2 + sigmas[j]**2)))\n",
    "    ent_lb_term += -pis[k] * tf.log(temp_val)\n",
    "    \n",
    "# entropy of mixture weights\n",
    "ent_mix_weights = 0.\n",
    "for k in range(max_n_loops):\n",
    "    ent_mix_weights += -pis[k] * tf.log(pis[k] + 1e-3)\n",
    "    \n",
    "# final objective\n",
    "objective_fn = tf.reduce_sum(expected_ll) + tf.reduce_sum(ent_lb_term) - tf.reduce_sum(ent_mix_weights)\n",
    "\n",
    "# collect parameters\n",
    "final_pis = tf.concat(2, [tf.expand_dims(t, 2) for t in pis])\n",
    "final_mus = tf.concat(2, [tf.expand_dims(t, 2) for t in mus])\n",
    "final_sigmas = tf.concat(2, [tf.expand_dims(t, 2) for t in sigmas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's train our dynamic RNN approximation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set training params\n",
    "n_epochs = 250\n",
    "learning_rate = .01\n",
    "\n",
    "# get the training operator\n",
    "train_model = tf.train.AdamOptimizer(learning_rate).minimize(-objective_fn, \\\n",
    "                    var_list=[params['h']['W'], params['h']['U'], params['h']['b'], \n",
    "                     params['mu']['W'], params['mu']['b'],\n",
    "                      params['sigma']['W'], params['sigma']['b'],\n",
    "                      params['pi']['W'], params['pi']['b']])\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    obj_tracker = 0.\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # perform update\n",
    "        _, obj = session.run([train_model, objective_fn], feed_dict={X: np.ones((5,1))})\n",
    "        obj_tracker += obj\n",
    "    \n",
    "        if (epoch_idx+1) % 10 == 0:\n",
    "            print \"Epoch %d.  VI Objective: %.3f\" %(epoch_idx+1, obj_tracker/10)\n",
    "            obj_tracker = 0.\n",
    "\n",
    "    m, s, p = session.run([final_mus, final_sigmas, final_pis], feed_dict={X: np.ones((5,1))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first axis just holds a fake mini-batch. Just throw it away..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu: [[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n",
      "Sigma: [[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n",
      "Pis: [[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "m, s, p = m[0, :, :], s[0, :, :], p[0, :, :]\n",
    "\n",
    "print \"Mu: \"+str(m)\n",
    "print \"Sigma: \"+str(s)\n",
    "print \"Pis: \"+str(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the approximation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x236756990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "probs_approx = [np.sum([p[0,k] * norm.pdf(z, loc=m[0, k], scale=s[0,k]) for k in range(max_n_loops)]) for z in z_grid]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(z_grid, probs_true, 'b-', linewidth=5, label=\"True Posterior\")\n",
    "plt.plot(z_grid, probs_approx, 'r-', linewidth=5, label=\"Approximation\")\n",
    "\n",
    "plt.xlabel(\"Z\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0,.25])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
